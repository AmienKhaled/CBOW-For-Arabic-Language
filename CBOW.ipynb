{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "\n",
    "if you don't have 'bidi' or 'arabic_reshaper' you have to install them as it will help print Arabic words in matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install arabic-reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils2 import sigmoid, get_batches, get_dict_mod, normalize_arabic, remove_diacritics, get_arabic_and_full_stop\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import arabic_reshaper\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = []\n",
    "root = '.\\\\data\\\\Khaleej-2004\\\\Economy'\n",
    "# get all files\n",
    "files_paths = [ os.path.join(path, name) for path, subdirs, files in os.walk(root) for name in files]\n",
    "\n",
    "# loop over files and process them, then append them to the data list\n",
    "for file_path in files_paths:\n",
    "\n",
    "    f = open(file_path, 'r', encoding='utf-8')\n",
    "\n",
    "    for line in f :\n",
    "        text = remove_diacritics(line)\n",
    "        text = normalize_arabic(text)\n",
    "        text = get_arabic_and_full_stop(text)\n",
    "        data += text\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:50])               #  print data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter wrong words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [word for word in data if len(set(word))>=2 or word == '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20)) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [word for word in data if fdist[word] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping words to indices and indices to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word2Ind = get_dict_mod(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2Ind['مصر']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 Training the Model\n",
    "\n",
    "###  Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Softmax\n",
    "Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n",
    "\n",
    "<br>\n",
    "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "\n",
    "    # Calculate yhat (softmax)\n",
    "    e_z = np.exp(z)\n",
    "    yhat = e_z/np.sum(e_z,axis=0)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Forward propagation\n",
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exercise 03\n",
    "Implement the forward propagation $z$ according to equations (1) to (3). <br>\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For that, you will use as activation the Rectified Linear Unit (ReLU) given by:\n",
    "\n",
    "$$f(h)=\\max (0,h) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "\n",
    "    # Calculate h\n",
    "    h = np.dot(W1,x)+b1\n",
    "    \n",
    "    # Apply the relu on h (store result in h)\n",
    "    h = np.maximum(0,h)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2,h)+b2\n",
    "    \n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "## 2.3 Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost functioN\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 Training the Model - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![back proba](imgs/back_porp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    \n",
    "    l1 = np.dot(W2.T,(yhat-y))\n",
    "    # Apply relu to l1\n",
    "    l1 = np.maximum(0,l1)\n",
    "    # Compute the gradient of W1\n",
    "    grad_W1 = (1/batch_size)*np.dot(l1,x.T)\n",
    "    # Compute the gradient of W2\n",
    "    grad_W2 = (1/batch_size)*np.dot(yhat-y,h.T)\n",
    "    # Compute the gradient of b1\n",
    "    grad_b1 = np.sum((1/batch_size)*np.dot(l1,x.T),axis=1,keepdims=True)\n",
    "    # Compute the gradient of b2\n",
    "    grad_b2 = np.sum((1/batch_size)*np.dot(yhat-y,h.T),axis=1,keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, word2Ind, N, V, num_iters, C=2, batch_size=128, alpha=0.03):\n",
    "\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n",
    "    iters = 0\n",
    "    \n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "\n",
    "        # Get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        # Get yhat\n",
    "        yhat = softmax(z)\n",
    "        # Get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 2 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        # Get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 -= alpha*grad_W1 \n",
    "        W2 -= alpha*grad_W2\n",
    "        b1 -= alpha*grad_b1\n",
    "        b2 -= alpha*grad_b2\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "C = 2\n",
    "batch_size = 128\n",
    "V = len(word2Ind)\n",
    "N = 300\n",
    "num_iters = 20\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = train(data, word2Ind, N, V, num_iters, C, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3.0 Visualizing the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['ملك', \n",
    "         'ملكه',\n",
    "         'رجل',\n",
    "         'امراه',\n",
    "         'طفل',\n",
    "         'طفله',\n",
    "         'حرب',\n",
    "         'سلام',\n",
    "         'الارض',\n",
    "         'السماء',\n",
    "         'الكواكب',\n",
    "         'النجوم',\n",
    "         'القمر',\n",
    "        ]\n",
    "\n",
    "idx = [word2Ind[word] for word in words]\n",
    "\n",
    "embs = (W1.T[idx, :] + W2[idx, :])/2.0\n",
    " \n",
    "X = embs\n",
    "print(X.shape, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [get_display(arabic_reshaper.reshape(word)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "pyplot.scatter(result[:, 1], result[:, 0])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 1], result[i, 0]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
